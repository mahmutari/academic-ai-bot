from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_message_histories import ChatMessageHistory

class ChatManager:
    def __init__(self, model_name="llama3.2"):
        # Temperature=0.3: TekrarÄ± Ã¶nlemek ve daha doÄŸal konuÅŸmak iÃ§in idealdir.
        self.llm = OllamaLLM(model=model_name, temperature=0.3)
        self.history = ChatMessageHistory()
        
    # llm_manager.py iÃ§indeki template bÃ¶lÃ¼mÃ¼:

        self.template = """
### ROL
Sen, hem genel akademik bilgilere sahip bir Ã¶ÄŸretmen hem de teknik dÃ¶kÃ¼manlarÄ± analiz eden uzman bir asistansÄ±n.

### TALÄ°MATLAR
1. Ã–NCELÄ°K KONTROLÃœ: KullanÄ±cÄ± dÃ¶kÃ¼manla (Matplotlib, Spyder, grafikler) ilgili bir ÅŸey soruyorsa, mutlaka BAÄLAM (Context) iÃ§indeki teknik bilgileri kullan.
2. DOÄRULAMA: EÄŸer bilgi baÄŸlamda VARSA, sakÄ±n "DÃ¶kÃ¼manda yok" deme. Bilgiyi dÃ¶kÃ¼mandan aldÄ±ÄŸÄ±nÄ± belirterek aÃ§Ä±kla.
3. GENEL BÄ°LGÄ°: EÄŸer soru dÃ¶kÃ¼manda bulunmayan tamamen farklÄ± bir akademik konuysa (Ã¶rneÄŸin "sÄ±fatlar", "zamirler"), kendi genel bilgilerini kullanarak detaylÄ± ve samimi bir aÃ§Ä±klama yap.
4. ÃœSLUP: Samimi, akademik ve net ol. Gereksiz tekrarlardan kaÃ§Ä±n.

BAÄLAM: {context}
GEÃ‡MÄ°Å: {chat_history}
SORU: {question}

CEVAP:
"""
        self.prompt = ChatPromptTemplate.from_template(self.template)

    def _format_history(self):
        return "\n".join([f"{'Ä°nsan' if m.type=='human' else 'AI'}: {m.content}" for m in self.history.messages[-4:]])

    def answer_question(self, question, retrieved_docs):
        # 1. BaÄŸlamÄ± ve dÃ¶kÃ¼man alaka kontrolÃ¼nÃ¼ hazÄ±rla
        context_text = ""
        is_relevant = False
        
        if retrieved_docs:
            context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])
            # Basit bir alaka kontrolÃ¼: Soru dÃ¶kÃ¼man anahtar kelimelerini iÃ§eriyor mu?
            keywords = ["plot", "matplotlib", "graph", "fig", "ax", "spyder", "Ã§izim", "grafik"]
            if any(key in question.lower() for key in keywords):
                is_relevant = True

        # 2. CevabÄ± Ã¼ret
        chain = self.prompt | self.llm
        response = chain.invoke({
            "context": context_text if is_relevant else "BU SORU Ä°Ã‡Ä°N DÃ–KÃœMAN KULLANMA.", 
            "chat_history": self._format_history(),
            "question": question
        })
        
        # 3. Kaynak GÃ¶sterme KararÄ±
        final_response = response
        if is_relevant and retrieved_docs:
            pages = sorted(list(set([doc.metadata.get('page', 0) + 1 for doc in retrieved_docs])))
            final_response = f"{response}\n\nğŸ“ (Kaynak: Sayfa {', '.join(map(str, pages))})"
        
        # 4. GeÃ§miÅŸi kaydet
        self.history.add_user_message(question)
        self.history.add_ai_message(final_response)
        return final_response